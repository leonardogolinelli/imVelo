{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import scanpy as sc\n",
    "import scvelo as scv\n",
    "import scib_metrics\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from plottable import ColumnDefinition, Table\n",
    "from plottable.cmap import normed_cmap\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_adatas = {}\n",
    "cell_type_key = \"clusters\"\n",
    "\n",
    "path = \"../../../outputs_old/final_anndatas/pancreas\"\n",
    "mivelo_path = f\"{path}/mivelo.h5ad\"\n",
    "celldancer_path = f\"{path}/celldancer.h5ad\"\n",
    "baseline_path = f\"{path}/baseline.h5ad\"\n",
    "scvelo_path = f\"{path}/scvelo.h5ad\"\n",
    "stochastic_path = f\"{path}/stochastic.h5ad\"\n",
    "ivelo_noproc_path = f\"{path}/ivelo_noproc.h5ad\"\n",
    "ivelo_proc_path = f\"{path}/ivelo_proc.h5ad\"\n",
    "velovi_noproc_path = f\"{path}/velovi_noproc.h5ad\"\n",
    "velovi_proc_path = f\"{path}/velovi_proc.h5ad\"\n",
    "expimap_path = f\"{path}/expimap.h5ad\"\n",
    "manifold_path = f\"{path}/manifold.h5ad\"\n",
    "\n",
    "paths = [mivelo_path, scvelo_path, stochastic_path, celldancer_path, velovi_noproc_path, ivelo_noproc_path,\n",
    "             velovi_proc_path, ivelo_proc_path, baseline_path, expimap_path]\n",
    "names = [\"mivelo\", \"scvelo\", \"steadystate_stochastic\", \"celldancer\", \"velovi_noproc\", \"ivelo_noproc\", \"velovi_proc\", \n",
    "         \"ivelo_proc\", \"baseline\", \"expimap\", \"dt\"]\n",
    "\n",
    "adata = sc.read_h5ad(paths[0])\n",
    "adata.obsm[\"z_mivelo\"] = adata.obsm[\"z\"]\n",
    "adata.obs[\"batch\"] = 0\n",
    "\n",
    "for i, path in enumerate(paths):\n",
    "    if names[i] in [\"velovi_noproc\",\"ivelo_noproc\", \"expimap\", \"dt\"]:\n",
    "        query_adata = sc.read_h5ad(path)\n",
    "        if names[i] == \"dt\":\n",
    "            adata.obsm[f\"z_{names[i]}\"] = query_adata.obsm[\"z5\"]\n",
    "        else:\n",
    "            adata.obsm[f\"z_{names[i]}\"] = query_adata.obsm[\"z\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_names = [\"z_mivelo\",\"z_velovi_noproc\",\"z_ivelo_noproc\", \"z_expimap\", \"z_dt\"]\n",
    "metric_names = [\"isolated_labels\",\"nmi_ari_cluster_labels_kmeans\", \"silhouette_label\", \"klisi_knn\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.write_h5ad(\"adata_latents.h5ad\")\n",
    "adata = sc.read_h5ad(\"adata_latents.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scib_metrics.benchmark\n",
    "from scib_metrics.benchmark import Benchmarker\n",
    "\n",
    "bio_metrics = scib_metrics.benchmark.BioConservation()\n",
    "batch_metrics = scib_metrics.benchmark.BatchCorrection(\n",
    "    False, False, False, False, False\n",
    ")\n",
    "\n",
    "bm = Benchmarker(\n",
    "    adata,\n",
    "    batch_key=\"batch\",\n",
    "    label_key=\"clusters\",\n",
    "    embedding_obsm_keys=z_names,\n",
    "    bio_conservation_metrics=bio_metrics,\n",
    "    batch_correction_metrics=batch_metrics,\n",
    "    n_jobs=1,\n",
    ")\n",
    "bm.benchmark()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The below methods are adapted from scib_metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def get_results( min_max_scale: bool = False, clean_names: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Return the benchmarking results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    min_max_scale\n",
    "        Whether to min max scale the results.\n",
    "    clean_names\n",
    "        Whether to clean the metric names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The benchmarking results.\n",
    "    \"\"\"\n",
    "    _METRIC_TYPE = \"Metric Type\"\n",
    "    # Mapping of metric fn names to clean DataFrame column names\n",
    "    metric_name_cleaner = {\n",
    "        \"silhouette_label\": \"Silhouette label\",\n",
    "        \"silhouette_batch\": \"Silhouette batch\",\n",
    "        \"isolated_labels\": \"Isolated labels\",\n",
    "        \"nmi_ari_cluster_labels_leiden_nmi\": \"Leiden NMI\",\n",
    "        \"nmi_ari_cluster_labels_leiden_ari\": \"Leiden ARI\",\n",
    "        \"nmi_ari_cluster_labels_kmeans_nmi\": \"KMeans NMI\",\n",
    "        \"nmi_ari_cluster_labels_kmeans_ari\": \"KMeans ARI\",\n",
    "        \"clisi_knn\": \"cLISI\",\n",
    "        \"ilisi_knn\": \"iLISI\",\n",
    "        \"kbet_per_label\": \"KBET\",\n",
    "        \"graph_connectivity\": \"Graph connectivity\",\n",
    "        \"pcr_comparison\": \"PCR comparison\",\n",
    "    }\n",
    "\n",
    "    df = bm._results.transpose()\n",
    "    df.index.name = \"Embedding\"\n",
    "    \n",
    "    df = df.loc[df.index != _METRIC_TYPE]\n",
    "    if min_max_scale:\n",
    "        # Use sklearn to min max scale\n",
    "        df = pd.DataFrame(\n",
    "            MinMaxScaler().fit_transform(df),\n",
    "            columns=df.columns,\n",
    "            index=df.index,\n",
    "        )\n",
    "    if clean_names:\n",
    "        df = df.rename(columns=metric_name_cleaner)\n",
    "    df = df.transpose()\n",
    "    df[_METRIC_TYPE] = bm._results[_METRIC_TYPE].values\n",
    "    df=df.transpose()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results_table(df, show: bool = True, save_dir: str = None) -> Table:\n",
    "    \"\"\"Plot the benchmarking results.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    show\n",
    "        Whether to show the plot.\n",
    "    save_dir\n",
    "        The directory to save the plot to. If `None`, the plot is not saved.\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "\n",
    "    _METRIC_TYPE = \"Metric Type\"\n",
    "    num_embeds = len(bm._embedding_obsm_keys)\n",
    "    cmap_fn = lambda col_data: normed_cmap(col_data, cmap=matplotlib.cm.PRGn, num_stds=2.5)\n",
    "    # Do not want to plot what kind of metric it is\n",
    "    plot_df = df.iloc[:-1, :]\n",
    "    # Sort by total score\n",
    "    # plot_df = plot_df.sort_values(by=\"Total\", ascending=False).astype(np.float64)\n",
    "    plot_df[\"Method\"] = plot_df.index\n",
    "\n",
    "    # Split columns by metric type, using df as it doesn't have the new method col\n",
    "    cols = df.columns\n",
    "    column_definitions = [\n",
    "        ColumnDefinition(\"Method\", width=4, textprops={\"ha\": \"left\", \"weight\": \"bold\"}),\n",
    "    ]\n",
    "    # Circles for the metric values\n",
    "    column_definitions += [\n",
    "        ColumnDefinition(\n",
    "            col,\n",
    "            title=col.replace(\" \", \"\\n\", 1),\n",
    "            width=1,\n",
    "            textprops={\n",
    "                \"ha\": \"center\",\n",
    "                \"bbox\": {\"boxstyle\": \"circle\", \"pad\": 0.25},\n",
    "            },\n",
    "            cmap=cmap_fn(plot_df[col]),\n",
    "            group=df.loc[_METRIC_TYPE, col],\n",
    "            formatter=\"{:.2f}\",\n",
    "        )\n",
    "        for i, col in enumerate(cols)\n",
    "    ]\n",
    "\n",
    "    with matplotlib.rc_context({\"svg.fonttype\": \"none\"}):\n",
    "        fig, ax = plt.subplots(figsize=(len(df.columns) * 1.25, 3 + 0.3 * num_embeds))\n",
    "        tab = Table(\n",
    "            plot_df,\n",
    "            cell_kw={\n",
    "                \"linewidth\": 0,\n",
    "                \"edgecolor\": \"k\",\n",
    "            },\n",
    "            column_definitions=column_definitions,\n",
    "            ax=ax,\n",
    "            row_dividers=True,\n",
    "            footer_divider=True,\n",
    "            textprops={\"fontsize\": 8, \"ha\": \"center\"},\n",
    "            row_divider_kw={\"linewidth\": 1, \"linestyle\": (0, (1, 5))},\n",
    "            col_label_divider_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "            column_border_kw={\"linewidth\": 1, \"linestyle\": \"-\"},\n",
    "            index_col=\"Method\",\n",
    "        ).autoset_fontcolors(colnames=plot_df.columns)\n",
    "    plt.show()\n",
    "    fig.savefig(\"benchmark.png\", facecolor=ax.get_facecolor(), dpi=300)\n",
    "\n",
    "    return tab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results_table(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepTrajectory",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
